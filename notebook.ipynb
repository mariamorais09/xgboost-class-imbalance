{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1753ef6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import os\n",
    "import random\n",
    "random.seed(111)\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import traceback\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, roc_auc_score, average_precision_score\n",
    "from scipy.stats import ttest_rel\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efecff4",
   "metadata": {},
   "source": [
    "#### **Implementation of the base algorithm for the Gradient Boosting Trees with direct inspiration in XGBoost**\n",
    "- We based our implementation on existing open-source code available at https://github.com/rushter/MLAlgorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "623f29c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseEstimator:\n",
    "    y_required = True\n",
    "    fit_required = True\n",
    "\n",
    "    def _setup_input(self, X, y=None):\n",
    "        \"\"\"Ensure inputs to an estimator are in the expected format.\n",
    "\n",
    "        Ensures X and y are stored as numpy ndarrays by converting from an\n",
    "        array-like object if necessary. Enables estimators to define whether\n",
    "        they require a set of y target values or not with y_required, e.g.\n",
    "        kmeans clustering requires no target labels and is fit against only X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like\n",
    "            Feature dataset.\n",
    "        y : array-like\n",
    "            Target values. By default is required, but if y_required = false\n",
    "            then may be omitted.\n",
    "        \"\"\"\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = np.array(X)\n",
    "\n",
    "        if X.size == 0:\n",
    "            raise ValueError(\"Got an empty matrix.\")\n",
    "\n",
    "        if X.ndim == 1:\n",
    "            self.n_samples, self.n_features = 1, X.shape\n",
    "        else:\n",
    "            self.n_samples, self.n_features = X.shape[0], np.prod(X.shape[1:])\n",
    "\n",
    "        self.X = X\n",
    "\n",
    "        if self.y_required:\n",
    "            if y is None:\n",
    "                raise ValueError(\"Missed required argument y\")\n",
    "\n",
    "            if not isinstance(y, np.ndarray):\n",
    "                y = np.array(y)\n",
    "\n",
    "            if y.size == 0:\n",
    "                raise ValueError(\"The targets array must be no-empty.\")\n",
    "\n",
    "        self.y = y\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self._setup_input(X, y)\n",
    "\n",
    "    def predict(self, X=None):\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = np.array(X)\n",
    "\n",
    "        if self.X is not None or not self.fit_required:\n",
    "            return self._predict(X)\n",
    "        else:\n",
    "            raise ValueError(\"You must call `fit` before `predict`\")\n",
    "\n",
    "    def _predict(self, X=None):\n",
    "        raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1df99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(X, y, value):\n",
    "    left_mask = X < value\n",
    "    right_mask = X >= value\n",
    "    return y[left_mask], y[right_mask]\n",
    "\n",
    "def get_split_mask(X, column, value):\n",
    "    left_mask = X[:, column] < value\n",
    "    right_mask = X[:, column] >= value\n",
    "    return left_mask, right_mask\n",
    "\n",
    "def split_dataset(X, target, column, value, return_X=True):\n",
    "    left_mask, right_mask = get_split_mask(X, column, value)\n",
    "\n",
    "    left, right = {}, {}\n",
    "    for key in target.keys():\n",
    "        left[key] = target[key][left_mask]\n",
    "        right[key] = target[key][right_mask]\n",
    "\n",
    "    if return_X:\n",
    "        left_X, right_X = X[left_mask], X[right_mask]\n",
    "        return left_X, right_X, left, right\n",
    "    else:\n",
    "        return left, right\n",
    "\n",
    "def xgb_criterion(y, left, right, loss):\n",
    "    left = loss.gain(left[\"actual\"], left[\"y_pred\"])\n",
    "    right = loss.gain(right[\"actual\"], right[\"y_pred\"])\n",
    "    initial = loss.gain(y[\"actual\"], y[\"y_pred\"])\n",
    "    gain = left + right - initial\n",
    "    return gain\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f55871",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree(object):\n",
    "    \"\"\"Recursive implementation of decision tree.\"\"\"\n",
    "\n",
    "    def __init__(self, criterion=None, n_classes=None):\n",
    "        self.impurity = None\n",
    "        self.threshold = None\n",
    "        self.column_index = None\n",
    "        self.outcome = None\n",
    "        self.criterion = criterion\n",
    "        self.loss = None\n",
    "        self.n_classes = n_classes  # Only for classification\n",
    "\n",
    "        self.left_child = None\n",
    "        self.right_child = None\n",
    "\n",
    "    @property\n",
    "    def is_terminal(self):\n",
    "        return not bool(self.left_child and self.right_child)\n",
    "\n",
    "    def _find_splits(self, X):\n",
    "        \"\"\"Find all possible split values.\"\"\"\n",
    "        split_values = set()\n",
    "\n",
    "        # Get unique values in a sorted order\n",
    "        x_unique = list(np.unique(X))\n",
    "        for i in range(1, len(x_unique)):\n",
    "            # Find a point between two values\n",
    "            average = (x_unique[i - 1] + x_unique[i]) / 2.0\n",
    "            split_values.add(average)\n",
    "\n",
    "        return list(split_values)\n",
    "\n",
    "    def _find_best_split(self, X, target, n_features):\n",
    "        \"\"\"Find best feature and value for a split. Greedy algorithm.\"\"\"\n",
    "\n",
    "        # Sample random subset of features\n",
    "        n_features = min(n_features, X.shape[1])  \n",
    "        subset = random.sample(list(range(0, X.shape[1])), n_features)\n",
    "        max_gain, max_col, max_val = None, None, None\n",
    "\n",
    "        for column in subset:\n",
    "            split_values = self._find_splits(X[:, column])\n",
    "            for value in split_values:\n",
    "            \n",
    "                left, right = split_dataset(X, target, column, value, return_X=False)\n",
    "            \n",
    "                if len(left[\"y\"]) == 0 or len(right[\"y\"]) == 0:\n",
    "                    continue\n",
    "                gain = xgb_criterion(target, left, right, self.loss)\n",
    "\n",
    "                if (max_gain is None) or (gain > max_gain):\n",
    "                    max_col, max_val, max_gain = column, value, gain\n",
    "\n",
    "        return max_col, max_val, max_gain\n",
    "\n",
    "\n",
    "    def _train(self, X, target, max_features=None, min_samples_split=10, max_depth=None, minimum_gain=0.0001):\n",
    "\n",
    "        try:\n",
    "            # Stops if few samples or zero depth\n",
    "            assert X.shape[0] > min_samples_split\n",
    "            assert max_depth > 0\n",
    "\n",
    "            if max_features is None:\n",
    "                max_features = X.shape[1]\n",
    "\n",
    "            column, value, gain = self._find_best_split(X, target, max_features)\n",
    "\n",
    "            if gain is None or gain <= minimum_gain:\n",
    "                raise AssertionError(\"Insuficient gain for split\")\n",
    "\n",
    "            self.column_index = column\n",
    "            self.threshold = value\n",
    "            self.impurity = gain\n",
    "\n",
    "            # Dataset split\n",
    "            left_X, right_X, left_target, right_target = split_dataset(X, target, column, value)\n",
    "\n",
    "\n",
    "            # Recursively grows children\n",
    "            self.left_child = Tree(self.criterion, self.n_classes)\n",
    "            self.left_child.loss = self.loss\n",
    "            self.left_child._train(\n",
    "                left_X, left_target, max_features, min_samples_split, max_depth - 1, minimum_gain\n",
    "            )\n",
    "\n",
    "            self.right_child = Tree(self.criterion, self.n_classes)\n",
    "            self.right_child.loss = self.loss\n",
    "            self.right_child._train(\n",
    "                right_X, right_target, max_features, min_samples_split, max_depth - 1, minimum_gain\n",
    "            )\n",
    "\n",
    "        except AssertionError:\n",
    "            self._calculate_leaf_value(target)\n",
    "\n",
    "\n",
    "    def train(self, X, target, max_features=None, min_samples_split=10, max_depth=None, minimum_gain=0.01, loss=None):\n",
    "        \"\"\"Build a decision tree from training set.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        X : array-like\n",
    "            Feature dataset.\n",
    "        target : dictionary or array-like\n",
    "            Target values.\n",
    "        max_features : int or None\n",
    "            The number of features to consider when looking for the best split.\n",
    "        min_samples_split : int\n",
    "            The minimum number of samples required to split an internal node.\n",
    "        max_depth : int\n",
    "            Maximum depth of the tree.\n",
    "        minimum_gain : float, default 0.01\n",
    "            Minimum gain required for splitting.\n",
    "        loss : function, default None\n",
    "            Loss function for gradient boosting.\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "\n",
    "        if not isinstance(target, dict):\n",
    "            target = {\"y\": target}\n",
    "\n",
    "        \n",
    "        self.loss = loss\n",
    "\n",
    "        self.n_classes = len(np.unique(target['y']))\n",
    "\n",
    "        self._train(X, target, max_features=max_features, min_samples_split=min_samples_split,\n",
    "                    max_depth=max_depth, minimum_gain=minimum_gain)\n",
    "\n",
    "\n",
    "    def _calculate_leaf_value(self, targets):\n",
    "        \"\"\"Find optimal value for leaf.\"\"\"\n",
    "        self.outcome = self.loss.approximate(targets[\"actual\"], targets[\"y_pred\"])\n",
    "\n",
    "    def predict_row(self, row):\n",
    "        \"\"\"Predict single row.\"\"\"\n",
    "        if not self.is_terminal:\n",
    "            if row.iloc[self.column_index] < self.threshold:\n",
    "                return self.left_child.predict_row(row)\n",
    "            else:\n",
    "                return self.right_child.predict_row(row)\n",
    "        return self.outcome\n",
    "\n",
    "    def predict(self, X):\n",
    "    # To garantee that X is a dataset\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X)\n",
    "\n",
    "        result = np.zeros(X.shape[0])\n",
    "        for i in range(X.shape[0]):\n",
    "            result[i] = self.predict_row(X.iloc[i, :])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe41c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    \"\"\"Base class for loss functions.\"\"\"\n",
    "\n",
    "    def __init__(self, regularization=1.0):\n",
    "        self.regularization = regularization\n",
    "\n",
    "    def grad(self, actual, predicted):\n",
    "        \"\"\"First order gradient.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def hess(self, actual, predicted):\n",
    "        \"\"\"Second order gradient.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def approximate(self, actual, predicted):\n",
    "        \"\"\"Approximate leaf value.\"\"\"\n",
    "        return self.grad(actual, predicted).sum() / (self.hess(actual, predicted).sum() + self.regularization)\n",
    "\n",
    "    def transform(self, pred):\n",
    "        \"\"\"Transform predictions values.\"\"\"\n",
    "        return pred\n",
    "\n",
    "    def gain(self, actual, predicted):\n",
    "        \"\"\"Calculate gain for split search.\"\"\"\n",
    "        nominator = self.grad(actual, predicted).sum() ** 2\n",
    "        denominator = self.hess(actual, predicted).sum() + self.regularization\n",
    "        return 0.5 * (nominator / denominator)\n",
    "\n",
    "\n",
    "\n",
    "class LogisticLoss(Loss):\n",
    "    \"\"\"Logistic loss.\"\"\"\n",
    "\n",
    "    def grad(self, actual, predicted):\n",
    "        return actual * expit(-actual * predicted)\n",
    "\n",
    "    def hess(self, actual, predicted):\n",
    "        expits = expit(predicted)\n",
    "        return expits * (1 - expits)\n",
    "\n",
    "    def transform(self, output):\n",
    "        # Apply logistic (sigmoid) function to the output\n",
    "        return expit(output)\n",
    "\n",
    "\n",
    "class GradientBoosting(BaseEstimator):\n",
    "    \"\"\"Gradient boosting trees with Taylor's expansion approximation (as in xgboost).\"\"\"\n",
    "    #classe central, implementa o algoritmo de boosting\n",
    "\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_features=10, max_depth=2, min_samples_split=10, loss = None):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.learning_rate = learning_rate #quão forte cada nova árvore influencia o modelo\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.n_estimators = n_estimators #nr de árvores (iterações de boosting)\n",
    "        self.trees = [] #lista de árvores treinadas\n",
    "        self.loss = loss #função de perda(classificação)\n",
    "\n",
    "    def fit(self, X, y=None): \n",
    "        self._setup_input(X, y) #vem da BaseEstimator, prepara os dados\n",
    "        self.y_mean = np.mean(y) #inicializa a média dos valores de y\n",
    "        self._train() #onde acontece o boosting\n",
    "\n",
    "    def _train(self): #coração do boosting\n",
    "        # Initialize model with zeros\n",
    "        y_pred = np.zeros(self.n_samples, np.float32)\n",
    "\n",
    "        for n in range(self.n_estimators):\n",
    "\n",
    "            residuals = self.loss.grad(self.y, y_pred) #calcula o gradiente(resíduo)\n",
    "            tree = Tree(criterion=xgb_criterion) #cria uma nova tree(árvore de decisão para classificação)\n",
    "            # Pass multiple target values to the tree learner\n",
    "            targets = { #define os targets especiais\n",
    "                # Residual values\n",
    "                \"y\": residuals,\n",
    "                # Actual target values\n",
    "                \"actual\": self.y,\n",
    "                # Predictions from previous step\n",
    "                \"y_pred\": y_pred,\n",
    "            }\n",
    "            tree.train( #treina a arvore com os dados anteriores\n",
    "                self.X,\n",
    "                targets,\n",
    "                max_features=self.max_features,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                max_depth=self.max_depth,\n",
    "                loss=self.loss,\n",
    "            )\n",
    "\n",
    "            #prediz com a nova árvore e atualiza y_pred\n",
    "            #Cada nova árvore corrige um pouco o erro das anteriores.\n",
    "            predictions = tree.predict(self.X)\n",
    "            y_pred += self.learning_rate * predictions\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def _predict(self, X=None):\n",
    "        y_pred = np.zeros(X.shape[0], np.float32)\n",
    "\n",
    "        for i, tree in enumerate(self.trees):\n",
    "            #soma as previsões de todas as árvores\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "        return y_pred\n",
    "\n",
    "    def predict(self, X=None):\n",
    "        #aplica transform, importante na classificação:\n",
    "        #transforma a predição bruta com a função logística sigmoid\n",
    "        #para obter probabilidade\n",
    "        return self.loss.transform(self._predict(X))\n",
    "\n",
    "\n",
    "class GradientBoostingClassifier(GradientBoosting):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(loss=LogisticLoss(), **kwargs)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        y = (y * 2) - 1  # transforma {0, 1} em {-1, 1}\n",
    "        super().fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0473adf1",
   "metadata": {},
   "source": [
    "#### **Challenge chosen: Class Imbalance**\n",
    "\n",
    "We pre-processed all 50 datasets so that:\n",
    "\n",
    "- No dataset was eliminated\n",
    "- Numerical attributes: missing values are filled with the mean\n",
    "- Categorical attributes: missing values are filled with the mode and then applied Label Encoder\n",
    "\n",
    "All testing of our algorithm is done on the pre-processed (cleaned) datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b33c722",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"class_imbalance/class_imbalance\"\n",
    "CLEANED_DIR = \"cleaned_datasets\"\n",
    "\n",
    "os.makedirs(CLEANED_DIR, exist_ok=True)\n",
    "\n",
    "def preprocess_data(df):\n",
    "    df = df.copy()\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        df[col] = df[col].fillna(df[col].mean())\n",
    "\n",
    "    for col in df.select_dtypes(include='object').columns:\n",
    "        df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "    for col in df.select_dtypes(include='object').columns:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col].astype(str))\n",
    "\n",
    "    return df\n",
    "\n",
    "def convert_target_to_zero_one(y):\n",
    "    \"\"\"Converte target binário qualquer (ex: [-1, 1]) para [0, 1]\"\"\"\n",
    "    classes = np.unique(y)\n",
    "    if len(classes) == 2 and not np.array_equal(classes, [0, 1]):\n",
    "        mapping = {classes[0]: 0, classes[1]: 1}\n",
    "        return y.map(mapping)\n",
    "    return y\n",
    "\n",
    "def save_cleaned_datasets():\n",
    "    for filename in os.listdir(DATA_DIR):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            try:\n",
    "                file_path = os.path.join(DATA_DIR, filename)\n",
    "                df = pd.read_csv(file_path)\n",
    "                df_clean = preprocess_data(df)\n",
    "\n",
    "                # Verifica se o target é binário\n",
    "                y = df_clean.iloc[:, -1]\n",
    "                if len(np.unique(y)) != 2:\n",
    "                    print(f\"Ignorado {filename}: target não é binário\")\n",
    "                    continue\n",
    "\n",
    "                # Converte target para [0, 1] se necessário\n",
    "                df_clean.iloc[:, -1] = convert_target_to_zero_one(df_clean.iloc[:, -1])\n",
    "\n",
    "                clean_name = filename.replace(\"dataset_\", \"\").replace(\".csv\", \"\") + \"_cleaned.csv\"\n",
    "                save_path = os.path.join(CLEANED_DIR, clean_name)\n",
    "                df_clean.to_csv(save_path, index=False)\n",
    "                print(f\"[SALVO] {save_path}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao processar {filename}: {e}\")\n",
    "                traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    save_cleaned_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b62c80f",
   "metadata": {},
   "source": [
    "#### **Testing the base algorithm**\n",
    "\n",
    "- To test the algorithm, we divided it into 3 folds, keeping the original proportion of classes in each fold (that is why it is stratified).  \n",
    "\n",
    "- The algorithm is trained with 2/3 of the data and tested on the remaining 1/3, repeating the process 3 times, every time with a different fold.\n",
    "\n",
    "- Then, for each fold we calculated the f1_score, ROC AUC and PR AUC. These result's mean if given as the final result for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53f8b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEANED_DIR = \"cleaned_datasets\"\n",
    "RESULTS_FILE = \"resultados_gbm_antes.csv\" \n",
    "\n",
    "\n",
    "def run_experiment_cleaned(file_path):\n",
    "    dataset_name = os.path.basename(file_path)\n",
    "    print(f\"\\nProcessando: {dataset_name}\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        X = df.iloc[:, :-1]\n",
    "        y = df.iloc[:, -1]\n",
    "\n",
    "        unique_classes = np.unique(y)\n",
    "        if len(unique_classes) != 2:\n",
    "            print(f\"Dataset ignorado ({dataset_name}): Target não é binário\")\n",
    "            return dataset_name, np.nan, np.nan, np.nan\n",
    "\n",
    "        # Definir a classe minoritária como positiva\n",
    "        class_counts = pd.Series(y).value_counts()\n",
    "        minor_class = class_counts.idxmin()\n",
    "\n",
    "        min_class_count = class_counts.min()\n",
    "        n_splits = min(3, min_class_count)\n",
    "        if n_splits < 2:\n",
    "            print(f\"Dataset ignorado ({dataset_name}): Classe minoritária muito pequena para validação cruzada\")\n",
    "            return dataset_name, np.nan, np.nan, np.nan\n",
    "\n",
    "        cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        f1_scores = []\n",
    "        auc_scores = []\n",
    "        pr_auc_scores = []\n",
    "\n",
    "        for fold, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n",
    "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "            model = GradientBoostingClassifier(n_estimators=50, max_depth=2, min_samples_split=5)\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            y_pred_proba = model.predict(X_test)\n",
    "            y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "            try:\n",
    "                f1 = f1_score(y_test, y_pred, pos_label=minor_class, average='binary')\n",
    "            except Exception as e:\n",
    "                print(f\"[F1 ERROR] {e}\")\n",
    "                f1 = np.nan\n",
    "            f1_scores.append(f1)\n",
    "\n",
    "            try:\n",
    "                auc = roc_auc_score((y_test == minor_class).astype(int), y_pred_proba)\n",
    "            except Exception as e:\n",
    "                print(f\"[AUC ERROR] {e}\")\n",
    "                auc = np.nan\n",
    "            auc_scores.append(auc)\n",
    "\n",
    "            try:\n",
    "                pr_auc = average_precision_score((y_test == minor_class).astype(int), y_pred_proba)\n",
    "            except Exception as e:\n",
    "                print(f\"[PR AUC ERROR] {e}\")\n",
    "                pr_auc = np.nan\n",
    "            pr_auc_scores.append(pr_auc)\n",
    "\n",
    "        return dataset_name, np.nanmean(f1_scores), np.nanmean(auc_scores), np.nanmean(pr_auc_scores)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar {dataset_name}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return dataset_name, np.nan, np.nan, np.nan\n",
    "\n",
    "\n",
    "def main():\n",
    "    results = []\n",
    "\n",
    "    for filename in os.listdir(CLEANED_DIR):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            path = os.path.join(CLEANED_DIR, filename)\n",
    "            result = run_experiment_cleaned(path)\n",
    "            results.append(result)\n",
    "\n",
    "    df_results = pd.DataFrame(results, columns=[\n",
    "        \"Dataset\", \n",
    "        \"F1 Score (minor class)\", \n",
    "        \"ROC AUC\", \n",
    "        \"PR AUC\"\n",
    "    ])\n",
    "    df_results.to_csv(RESULTS_FILE, index=False)\n",
    "    print(f\"\\nExperimentos finalizados. Resultados salvos em '{RESULTS_FILE}'.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa9596b",
   "metadata": {},
   "source": [
    "#### **Applying changes to make the algorithm more robust to class imbalance**\n",
    "\n",
    "- **Hellinger Distance Gain:** Replaced default gain function in the first tree with Hellinger gain, encourages splits that highlight minority patterns. Used only in the first tree to guide initial splits while keeping the full power of XGBoost in subsequent trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b7c0c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hellinger_gain(y, splits):\n",
    "    def hellinger(p, q):\n",
    "        return np.sqrt(np.sum((np.sqrt(p) - np.sqrt(q)) ** 2)) / np.sqrt(2)\n",
    "\n",
    "    def class_prob(y):\n",
    "        counts = np.bincount(y.astype(int),minlength=2)  #(para as classes 0 e 1), y está em float então usamos em int\n",
    "        total = counts.sum()\n",
    "        return counts / total if total > 0 else np.zeros_like(counts)\n",
    "\n",
    "    p = class_prob(y)\n",
    "    gain = 0.0\n",
    "    for split in splits:\n",
    "        q = class_prob(split)\n",
    "        gain += len(split) / len(y) * hellinger(p, q)\n",
    "\n",
    "    return gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb45ed9",
   "metadata": {},
   "source": [
    "- **Class Weights:** Assigns directly in the loss function higher weight to errors on the minority class, based on inverse class frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56588713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_criterion(y, left, right, loss, class_weights=None):\n",
    "    left_gain = loss.gain(left[\"actual\"], left[\"y_pred\"], class_weights)\n",
    "    right_gain = loss.gain(right[\"actual\"], right[\"y_pred\"], class_weights)\n",
    "    initial_gain = loss.gain(y[\"actual\"], y[\"y_pred\"], class_weights)\n",
    "    return left_gain + right_gain - initial_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5a00b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updating the Tree Class to accept weighted trees\n",
    "\n",
    "class Tree(object):\n",
    "\n",
    "    def __init__(self, criterion=None, n_classes=None,class_weights=None):\n",
    "        self.impurity = None\n",
    "        self.threshold = None\n",
    "        self.column_index = None\n",
    "        self.outcome = None\n",
    "        self.criterion = criterion\n",
    "        self.loss = None\n",
    "        self.n_classes = n_classes \n",
    "        self.class_weights=class_weights #here is the new parameter\n",
    "\n",
    "        self.left_child = None\n",
    "        self.right_child = None\n",
    "\n",
    "    @property\n",
    "    def is_terminal(self):\n",
    "        return not bool(self.left_child and self.right_child)\n",
    "\n",
    "    def _find_splits(self, X):\n",
    "        \"\"\"Find all possible split values.\"\"\"\n",
    "        split_values = set()\n",
    "\n",
    "        # Get unique values in a sorted order\n",
    "        x_unique = list(np.unique(X))\n",
    "        for i in range(1, len(x_unique)):\n",
    "            # Find a point between two values\n",
    "            average = (x_unique[i - 1] + x_unique[i]) / 2.0\n",
    "            split_values.add(average)\n",
    "\n",
    "        return list(split_values)\n",
    "\n",
    "    def _find_best_split(self, X, target, n_features):\n",
    "        \"\"\"Find best feature and value for a split. Greedy algorithm.\"\"\"\n",
    "\n",
    "        # Sample random subset of features\n",
    "        n_features = min(n_features, X.shape[1]) \n",
    "        subset = random.sample(list(range(0, X.shape[1])), n_features)\n",
    "        max_gain, max_col, max_val = None, None, None\n",
    "\n",
    "        for column in subset:\n",
    "            split_values = self._find_splits(X[:, column])\n",
    "            for value in split_values:\n",
    "            \n",
    "                left, right = split_dataset(X, target, column, value, return_X=False)\n",
    "            \n",
    "                if len(left[\"y\"]) == 0 or len(right[\"y\"]) == 0:\n",
    "                    continue\n",
    "                gain = xgb_criterion(target, left, right, self.loss,self.class_weights)\n",
    "\n",
    "                if (max_gain is None) or (gain > max_gain):\n",
    "                    max_col, max_val, max_gain = column, value, gain\n",
    "\n",
    "        return max_col, max_val, max_gain\n",
    "\n",
    "\n",
    "    def _train(self, X, target, max_features=None, min_samples_split=10, max_depth=None, minimum_gain=0.0001, class_weights=None):\n",
    "\n",
    "        try:\n",
    "            # Interrompe se poucas amostras ou profundidade zerada\n",
    "            assert X.shape[0] > min_samples_split\n",
    "            assert max_depth > 0\n",
    "\n",
    "            if max_features is None:\n",
    "                max_features = X.shape[1]\n",
    "\n",
    "            column, value, gain = self._find_best_split(X, target, max_features)\n",
    "\n",
    "            if gain is None or gain <= minimum_gain:\n",
    "                raise AssertionError(\"Ganho insuficiente para split\")\n",
    "\n",
    "            self.column_index = column\n",
    "            self.threshold = value\n",
    "            self.impurity = gain\n",
    "\n",
    "            # Divisão do dataset\n",
    "            left_X, right_X, left_target, right_target = split_dataset(X, target, column, value)\n",
    "\n",
    "\n",
    "            # Cresce filhos recursivamente\n",
    "            self.left_child = Tree(self.criterion, self.n_classes,self.class_weights)\n",
    "            self.left_child.loss = self.loss\n",
    "            self.left_child._train(\n",
    "                left_X, left_target, max_features, min_samples_split, max_depth - 1, minimum_gain, self.class_weights\n",
    "            )\n",
    "\n",
    "            self.right_child = Tree(self.criterion, self.n_classes, self.class_weights)\n",
    "            self.right_child.loss = self.loss\n",
    "            self.right_child._train(\n",
    "                right_X, right_target, max_features, min_samples_split, max_depth - 1, minimum_gain, self.class_weights\n",
    "            )\n",
    "\n",
    "        except AssertionError:\n",
    "            self._calculate_leaf_value(target)\n",
    "\n",
    "\n",
    "    def train(self, X, target, max_features=None, min_samples_split=10, max_depth=None, minimum_gain=0.01, loss=None,class_weights=None):\n",
    "        \"\"\"Build a decision tree from training set.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        X : array-like\n",
    "            Feature dataset.\n",
    "        target : dictionary or array-like\n",
    "            Target values.\n",
    "        max_features : int or None\n",
    "            The number of features to consider when looking for the best split.\n",
    "        min_samples_split : int\n",
    "            The minimum number of samples required to split an internal node.\n",
    "        max_depth : int\n",
    "            Maximum depth of the tree.\n",
    "        minimum_gain : float, default 0.01\n",
    "            Minimum gain required for splitting.\n",
    "        loss : function, default None\n",
    "            Loss function for gradient boosting.\n",
    "        \"\"\"\n",
    "\n",
    "        if not isinstance(target, dict):\n",
    "            target = {\"y\": target}\n",
    "\n",
    "        self.loss = loss\n",
    "\n",
    "        self.n_classes = len(np.unique(target['y']))\n",
    "\n",
    "        self._train(X, target, max_features=max_features, min_samples_split=min_samples_split,\n",
    "                max_depth=max_depth, minimum_gain=minimum_gain, class_weights=class_weights)\n",
    "\n",
    "    def _calculate_leaf_value(self, targets):\n",
    "        \"\"\"Find optimal value for leaf.\"\"\"\n",
    "        self.outcome = self.loss.approximate(targets[\"actual\"], targets[\"y_pred\"])\n",
    "\n",
    "    def predict_row(self, row):\n",
    "        \"\"\"Predict single row.\"\"\"\n",
    "        if not self.is_terminal:\n",
    "            if row.iloc[self.column_index] < self.threshold:\n",
    "                return self.left_child.predict_row(row)\n",
    "            else:\n",
    "                return self.right_child.predict_row(row)\n",
    "        return self.outcome\n",
    "\n",
    "    def predict(self, X):\n",
    "    # Garante que X seja sempre um DataFrame\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X)\n",
    "\n",
    "        result = np.zeros(X.shape[0])\n",
    "        for i in range(X.shape[0]):\n",
    "            result[i] = self.predict_row(X.iloc[i, :])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424ac9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updating the Loss Classes for the new changes\n",
    "\n",
    "class Loss:\n",
    "    \"\"\"Base class for loss functions.\"\"\"\n",
    "\n",
    "    def __init__(self, regularization=1.0):\n",
    "        self.regularization = regularization\n",
    "\n",
    "    def grad(self, actual, predicted): #gradiente da perda (1ª derivada)\n",
    "        \"\"\"First order gradient.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def hess(self, actual, predicted): # hessiana (2ª derivada)\n",
    "        \"\"\"Second order gradient.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def approximate(self, actual, predicted): # valor ótimo da folha baseado nas derivadas (como no XGBoost)\n",
    "        \"\"\"Approximate leaf value.\"\"\"\n",
    "        return self.grad(actual, predicted).sum() / (self.hess(actual, predicted).sum() + self.regularization)\n",
    "\n",
    "    def transform(self, pred):\n",
    "        \"\"\"Transform predictions values.\"\"\"\n",
    "        return pred\n",
    "\n",
    "    def gain(self, actual, predicted,class_weights=None): #mede quanto \"ganho de informação\" temos ao fazer um split\n",
    "        \"\"\"Calculate gain for split search.\"\"\"\n",
    "        nominator = self.grad(actual, predicted).sum() ** 2\n",
    "        denominator = self.hess(actual, predicted).sum() + self.regularization\n",
    "        return 0.5 * (nominator / denominator)\n",
    "\n",
    "\n",
    "class LogisticLoss(Loss): #usada para classificação binária (com a função logística/sigmoid)\n",
    "    \"\"\"Logistic loss.\"\"\"\n",
    "    \n",
    "    def __init__(self, class_weights=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "        \n",
    "    def _safe_index(self, actual):\n",
    "        # Garante que os valores estejam em {0, 1}\n",
    "        if np.any((actual != 0) & (actual != 1)):\n",
    "            return ((actual + 1) // 2).astype(int)\n",
    "        else:\n",
    "            return actual.astype(int)\n",
    "\n",
    "    def grad(self, actual, predicted):\n",
    "        # Pesos de classe para cada instância\n",
    "        class_weight = self.class_weights[self._safe_index(actual)]\n",
    "        return class_weight * actual * expit(-actual * predicted)\n",
    "    \n",
    "    def hess(self, actual, predicted):\n",
    "        expits = expit(predicted)\n",
    "        class_weight = self.class_weights[self._safe_index(actual)]\n",
    "        return class_weight * expits * (1 - expits)\n",
    "\n",
    "    def transform(self, output):\n",
    "        # Apply logistic (sigmoid) function to the output\n",
    "        return expit(output)\n",
    "\n",
    "\n",
    "class GradientBoosting(BaseEstimator):\n",
    "    \"\"\"Gradient boosting trees with Taylor's expansion approximation (as in xgboost).\"\"\"\n",
    "    #classe central, implementa o algoritmo de boosting\n",
    "\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_features=10, max_depth=2, min_samples_split=10):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.learning_rate = learning_rate #quão forte cada nova árvore influencia o modelo\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.n_estimators = n_estimators #nr de árvores (iterações de boosting)\n",
    "        self.trees = [] #lista de árvores treinadas\n",
    "        self.loss = None #função de perda, definida depois(classificação)\n",
    "\n",
    "    def fit(self, X, y=None): \n",
    "        self._setup_input(X, y) #vem da BaseEstimator, prepara os dados\n",
    "        self.y_mean = np.mean(y) #inicializa a média dos valores de y\n",
    "        self._train() #onde acontece o boosting\n",
    "\n",
    "\n",
    "    def _train(self): #coração do boosting\n",
    "        # Initialize model with zeros\n",
    "        y_pred = np.zeros(self.n_samples, np.float32)\n",
    "        \n",
    "        for n in range(self.n_estimators):\n",
    "            residuals = self.loss.grad(self.y, y_pred) #calcula o gradiente(resíduo)\n",
    "            \n",
    "            if n==0:\n",
    "                if self.use_hellinger:\n",
    "                    criterion = hellinger_gain #primeira árvore usa hellinger\n",
    "                else:\n",
    "                    self.y = (self.y * 2) - 1 #não queremos usar hellinger de todo\n",
    "                    criterion = lambda y, l, r, loss: xgb_criterion(y, l, r, loss, class_weights=self.loss.class_weights) #as outras usam o critério baseado no loss\n",
    "\n",
    "                \n",
    "            else:\n",
    "                criterion = lambda y, l, r, loss: xgb_criterion(y, l, r, loss, class_weights=self.loss.class_weights) #as outras usam o critério baseado no loss\n",
    "                \n",
    "            tree = Tree(criterion=criterion, class_weights=self.loss.class_weights)\n",
    "            # Pass multiple target values to the tree learner\n",
    "            targets = { #define os targets especiais\n",
    "                # Residual values\n",
    "                \"y\": residuals,\n",
    "                # Actual target values\n",
    "                \"actual\": self.y,\n",
    "                # Predictions from previous step\n",
    "                \"y_pred\": y_pred,\n",
    "            }\n",
    "            tree.train( #treina a arvore com os dados anteriores\n",
    "                self.X,\n",
    "                targets,\n",
    "                max_features=self.max_features,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                max_depth=self.max_depth,\n",
    "                loss=self.loss,\n",
    "            )\n",
    "\n",
    "            #prediz com a nova árvore e atualiza y_pred\n",
    "            #Cada nova árvore corrige um pouco o erro das anteriores.\n",
    "            predictions = tree.predict(self.X)\n",
    "            y_pred += self.learning_rate * predictions\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "            # Na primeira árvore, usa y na forma {0,1}, a partir daí usa y {-1,1}\n",
    "            if n == 0 and self.use_hellinger:\n",
    "                self.y = (self.y * 2) - 1\n",
    "\n",
    "    def _predict(self, X=None): \n",
    "        y_pred = np.zeros(X.shape[0], np.float32)\n",
    "\n",
    "        for i, tree in enumerate(self.trees):\n",
    "            #soma as previsões de todas as árvores\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "        return y_pred\n",
    "\n",
    "    def predict(self, X=None):\n",
    "        #aplica transform, importante na classificação:\n",
    "        #transforma a predição bruta com a função logística sigmoid\n",
    "        #para obter probabilidade\n",
    "        return self.loss.transform(self._predict(X))\n",
    "        \n",
    "class GradientBoostingClassifier(GradientBoosting):\n",
    "    def __init__(self, *, use_weights=True, use_hellinger=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.use_weights = use_weights\n",
    "        self.use_hellinger = use_hellinger\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        y = y.astype(int)\n",
    "        \n",
    "        if self.use_weights:\n",
    "            # calcular pesos para cada classe\n",
    "            class_weights = self.calculate_class_weights(y)\n",
    "        else:\n",
    "            # peso uniforme(neutro) \n",
    "            class_weights = np.ones(2, dtype=float)\n",
    "        self.loss = LogisticLoss(class_weights=class_weights)\n",
    "\n",
    "        # Passar os pesos como argumento\n",
    "        super(GradientBoostingClassifier, self).fit(X, y)\n",
    "        \n",
    "    def calculate_class_weights(self, y):\n",
    "        \"\"\"Calcula pesos inversamente proporcionais à frequência das classes.\"\"\"\n",
    "        class_counts = np.bincount(y)\n",
    "        total_samples = len(y)\n",
    "        # Pesos inversamente proporcionais à frequência das classes\n",
    "        weights = total_samples / (len(class_counts) * class_counts)\n",
    "        return weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870c3d52",
   "metadata": {},
   "source": [
    "#### **Testing the modified algorithm**\n",
    "\n",
    "- To do so, we used the same logic as above\n",
    "- Just by changing the parameters \"use_weights\" and \"use_splits\" of the GradientBoostingClassifier function we could test the algorithm with just one of both modifications and save it in the respective csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03b4f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEANED_DIR = \"cleaned_datasets\"\n",
    "RESULTS_FILE = \"resultados_gbm_ignorar.csv\"\n",
    "\n",
    "def run_experiment_cleaned(file_path):\n",
    "    dataset_name = os.path.basename(file_path)\n",
    "    print(f\"\\nProcessando: {dataset_name}\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        X = df.iloc[:, :-1]\n",
    "        y = df.iloc[:, -1]\n",
    "\n",
    "        unique_classes = np.unique(y)\n",
    "        if len(unique_classes) != 2:\n",
    "            print(f\"Dataset ignorado ({dataset_name}): Target não é binário\")\n",
    "            return dataset_name, np.nan, np.nan\n",
    "\n",
    "        # Definir a classe minoritária como positiva\n",
    "        class_counts = pd.Series(y).value_counts()\n",
    "        minor_class = class_counts.idxmin()\n",
    "\n",
    "        min_class_count = class_counts.min()\n",
    "        n_splits = min(3, min_class_count)\n",
    "        if n_splits < 2:\n",
    "            print(f\"Dataset ignorado ({dataset_name}): Classe minoritária muito pequena para validação cruzada\")\n",
    "            return dataset_name, np.nan, np.nan\n",
    "\n",
    "        cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        f1_scores = []\n",
    "        auc_scores = []\n",
    "        pr_auc_scores = []\n",
    "\n",
    "        for fold, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n",
    "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "            model = GradientBoostingClassifier(n_estimators=50, max_depth=2, min_samples_split=5,use_weights=True, use_hellinger=True)\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            y_pred_proba = model.predict(X_test)\n",
    "            y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "            # F1 usando a classe minoritária como positiva\n",
    "            try:\n",
    "                f1 = f1_score(y_test, y_pred, pos_label=minor_class, average='binary')\n",
    "            except Exception as e:\n",
    "                print(f\"[F1 ERROR] {e}\")\n",
    "                f1 = np.nan\n",
    "            f1_scores.append(f1)\n",
    "\n",
    "            try:\n",
    "                auc = roc_auc_score((y_test == minor_class).astype(int), y_pred_proba)\n",
    "            except Exception as e:\n",
    "                print(f\"[AUC ERROR] {e}\")\n",
    "                auc = np.nan\n",
    "            auc_scores.append(auc)\n",
    "\n",
    "            try:\n",
    "                pr_auc = average_precision_score((y_test == minor_class).astype(int), y_pred_proba)\n",
    "            except Exception as e:\n",
    "                print(f\"[PR AUC ERROR] {e}\")\n",
    "                pr_auc = np.nan\n",
    "            pr_auc_scores.append(pr_auc)\n",
    "\n",
    "        return dataset_name, np.nanmean(f1_scores), np.nanmean(auc_scores), np.nanmean(pr_auc_scores)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar {dataset_name}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return dataset_name, np.nan, np.nan\n",
    "\n",
    "\n",
    "def main():\n",
    "    results = []\n",
    "\n",
    "    for filename in os.listdir(CLEANED_DIR):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            path = os.path.join(CLEANED_DIR, filename)\n",
    "            result = run_experiment_cleaned(path)\n",
    "            results.append(result)\n",
    "\n",
    "    df_results = pd.DataFrame(results, columns=[\"Dataset\", \"F1 Score (minor class)\", \"ROC AUC\", \"PR AUC\"])\n",
    "    df_results.to_csv(RESULTS_FILE, index=False)\n",
    "    print(f\"\\nExperimentos finalizados. Resultados salvos em '{RESULTS_FILE}'.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af45235f",
   "metadata": {},
   "source": [
    "#### **Comparing our modified algorithm with the one available in sklearn**\n",
    "\n",
    "- To complement our work, we tested the sklearn algorithm on our 50 datasets and generated the results for the same metrics as above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77de236b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "cleaned_dir = \"cleaned_datasets\"\n",
    "sklearn_csv = \"sklearn_comparison_results.csv\"\n",
    "\n",
    "\n",
    "def evaluate_sklearn(dataset_path):\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1].astype(int)\n",
    "    \n",
    "    # Class imbalance handling\n",
    "    class_counts = np.bincount(y)\n",
    "    minor_class = np.argmin(class_counts)\n",
    "    \n",
    "    # Match your original stratified 3-fold logic\n",
    "    n_splits = min(3, class_counts.min())\n",
    "    if n_splits < 2:\n",
    "        return os.path.basename(dataset_path), np.nan, np.nan, np.nan\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    model = GradientBoostingClassifier(\n",
    "        n_estimators=50,\n",
    "        max_depth=2,\n",
    "        min_samples_split=5,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        random_state=111,\n",
    "        verbose=0\n",
    "    )\n",
    "    f1_scores, roc_auc_scores, pr_auc_scores = [], [], []\n",
    "    \n",
    "    for train_idx, test_idx in cv.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "        y_pred = (y_proba >= 0.5).astype(int)\n",
    "        \n",
    "        f1 = f1_score(y_test, y_pred, pos_label=minor_class)\n",
    "        roc_auc = roc_auc_score(y_test, y_proba)\n",
    "        pr_auc = average_precision_score(y_test, y_proba)\n",
    "        \n",
    "        f1_scores.append(f1)\n",
    "        roc_auc_scores.append(roc_auc)\n",
    "        pr_auc_scores.append(pr_auc)\n",
    "    \n",
    "    return (\n",
    "        os.path.basename(dataset_path),\n",
    "        np.nanmean(f1_scores),\n",
    "        np.nanmean(roc_auc_scores),\n",
    "        np.nanmean(pr_auc_scores)\n",
    "    )\n",
    "\n",
    "def run_comparison(cleaned_dir, output_csv):\n",
    "    results = []\n",
    "    for filename in os.listdir(cleaned_dir):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            path = os.path.join(cleaned_dir, filename)\n",
    "            result = evaluate_sklearn(path)\n",
    "            results.append(result)\n",
    "    df = pd.DataFrame(results, columns=[\"Dataset\", \"F1 Score (minor class)\", \"ROC AUC\", \"PR AUC\"])\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"[SAVED] {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac35986",
   "metadata": {},
   "source": [
    "#### **Comparison between the different results**\n",
    "\n",
    "- We compute not only the difference mean, median and Std Deviation between the results before and after the modifications but also the p-value for the paired t-test\n",
    "- We also use the results from the sklearn algorithm to check wheter our is statistically better or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d120d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_antigo = pd.read_csv(\"resultados_gbm_antes.csv\")\n",
    "#df_novo = pd.read_csv(\"resultados_gbm_splits.csv\")\n",
    "#df_novo = pd.read_csv(\"resultados_gbm_weights.csv\")\n",
    "df_novo = pd.read_csv(\"resultados_gbm_weights_splits.csv\")\n",
    "df_antigo = pd.read_csv(\"sklearn_comparison_results.csv\")\n",
    "\n",
    "#Make sure the datasets have the same order\n",
    "df_antigo = df_antigo.sort_values(\"Dataset\").reset_index(drop=True)\n",
    "df_novo = df_novo.sort_values(\"Dataset\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "assert all(df_antigo[\"Dataset\"] == df_novo[\"Dataset\"]), \"Datasets não estão alinhados!\"\n",
    "\n",
    "#Calculate the differences\n",
    "df_diff = df_novo[[\"F1 Score (minor class)\", \"ROC AUC\", \"PR AUC\"]] - df_antigo[[\"F1 Score (minor class)\", \"ROC AUC\", \"PR AUC\"]]\n",
    "\n",
    "df_diff[\"Dataset\"] = df_novo[\"Dataset\"]\n",
    "\n",
    "\n",
    "print(\"\\n=== Difference Mean ===\")\n",
    "print(df_diff[[\"F1 Score (minor class)\", \"ROC AUC\", \"PR AUC\"]].mean())\n",
    "\n",
    "print(\"\\n=== Difference Median ===\")\n",
    "print(df_diff[[\"F1 Score (minor class)\", \"ROC AUC\", \"PR AUC\"]].median())\n",
    "\n",
    "print(\"\\n=== Difference Std Deviation ===\")\n",
    "print(df_diff[[\"F1 Score (minor class)\", \"ROC AUC\", \"PR AUC\"]].std())\n",
    "\n",
    "print(\"\\n=== Nº of instances that the new algorithm as better results ===\")\n",
    "print((df_diff[[\"F1 Score (minor class)\", \"ROC AUC\", \"PR AUC\"]] > 0).sum())\n",
    "\n",
    "print(\"\\n=== Nº of cases that the new  algorithm has worst results ===\")\n",
    "print((df_diff[[\"F1 Score (minor class)\", \"ROC AUC\", \"PR AUC\"]] < 0).sum())\n",
    "\n",
    "#Statistical testing\n",
    "print(\"\\n=== Paired t-test (p-value) ===\")\n",
    "for col in [\"F1 Score (minor class)\", \"ROC AUC\", \"PR AUC\"]:\n",
    "    t_stat, p_value = ttest_rel(df_novo[col], df_antigo[col])\n",
    "    print(f\"{col}: p = {p_value:.4f}\")\n",
    "\n",
    "#Plot the results\n",
    "df_diff[[\"F1 Score (minor class)\", \"ROC AUC\", \"PR AUC\"]].plot(kind=\"box\", title=\"Difference Boxplot\")\n",
    "plt.ylabel(\"Difference (new - old)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot comparativo \n",
    "plt.figure(figsize=(12, 4))\n",
    "for i, col in enumerate([\"F1 Score (minor class)\", \"ROC AUC\", \"PR AUC\"]):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.scatter(df_antigo[col], df_novo[col], alpha=0.7)\n",
    "    plt.plot([0, 1], [0, 1], \"--\", color=\"gray\")\n",
    "    plt.xlabel(\"Antigo\")\n",
    "    plt.ylabel(\"Novo\")\n",
    "    plt.title(col)\n",
    "    plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
